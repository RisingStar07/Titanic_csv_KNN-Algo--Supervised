{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Titanic-KNN.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import itertools\n",
        "import random \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "random.seed(0)\n",
        "\n",
        "# Read the data \n",
        "train_ws = pd.read_csv('train.csv')\n",
        "test_ws =  pd.read_csv('test.csv')\n",
        "\n",
        "# Join the train and test dataframes so the data preprocessing will be done simultaneously in both datasets \n",
        "full_ws = train_ws.append(test_ws, ignore_index=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:35.804279Z",
          "iopub.execute_input": "2021-10-21T19:21:35.804844Z",
          "iopub.status.idle": "2021-10-21T19:21:35.835481Z",
          "shell.execute_reply.started": "2021-10-21T19:21:35.8048Z",
          "shell.execute_reply": "2021-10-21T19:21:35.83448Z"
        },
        "trusted": true,
        "id": "ylgDUTWvPBSb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(ws):\n",
        "  \n",
        "  # Label-encode the sex of a passenger \n",
        "  ws['Sex'] = ws['Sex'].replace(['male'],0)\n",
        "  ws['Sex'] = ws['Sex'].replace(['female'],1)\n",
        "\n",
        "  # Initialize new columns \n",
        "  ws['title'] = np.NaN\n",
        "  ws['alone'] = np.NaN\n",
        "  ws['cabin_class'] = np.NaN\n",
        "\n",
        "  # Identify if a passenger is alone in the ship \n",
        "  for i,_ in enumerate(ws['alone']):\n",
        "    if ws['SibSp'][i] + ws['Parch'][i] == 0:\n",
        "      ws['alone'][i] = 1\n",
        "    else:\n",
        "      ws['alone'][i] = 0 \n",
        "\n",
        "  # Handle missing values\n",
        "  cols = ['SibSp','Parch','Fare','Age']\n",
        "  for col in cols:\n",
        "    ws[col].fillna(ws[col].median(), inplace = True)\n",
        "    \n",
        "  # Feature-engineer the cabin-class \n",
        "  for i,row in enumerate(ws['Cabin']):\n",
        "    # Get cabin class \n",
        "    ws['cabin_class'][i] =  str(row)[:1]\n",
        "\n",
        "  # Count the cabin distribution per class (if available) \n",
        "  cabin_distribution = {}\n",
        "  count = 0 \n",
        "  for row in ws['cabin_class']:\n",
        "    if row != 'n':\n",
        "      count += 1 \n",
        "      if row not in cabin_distribution:\n",
        "        cabin_distribution[row] = 1 \n",
        "      else:\n",
        "        cabin_distribution[row] +=1 \n",
        "\n",
        "  # Calculate the probability of being in a sepcific cabin-class  \n",
        "  cabin_pws = {k:v / count for k, v in cabin_distribution.items()}\n",
        "\n",
        "  # Calculate the cumulative probability of being in a specific cabin-class \n",
        "  keys, vals = cabin_pws.keys(), cabin_pws.values()\n",
        "  cabin_cws = dict(zip(keys, itertools.accumulate(vals)))\n",
        "  cabin_cws = sorted(cabin_cws.items(), key=lambda x: x[1])    \n",
        "\n",
        "  # Randomly assign cabin-classes to passengers that are missing the cabin \n",
        "  # field, based on the probabilities calculated above \n",
        "  for i,row in enumerate(ws['cabin_class']):\n",
        "    random_num = random.random()\n",
        "    if row == 'n':\n",
        "      if random_num < cabin_cws[0][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[0][0]\n",
        "      elif cabin_cws[0][1] <= random_num < cabin_cws[1][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[1][0]\n",
        "\n",
        "      elif cabin_cws[1][1] <= random_num < cabin_cws[2][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[2][0]\n",
        "      \n",
        "      elif cabin_cws[2][1] <= random_num < cabin_cws[3][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[2][0]\n",
        "\n",
        "      elif cabin_cws[3][1] <= random_num < cabin_cws[4][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[3][0]\n",
        "\n",
        "      elif cabin_cws[3][1] <= random_num < cabin_cws[4][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[4][0]\n",
        "\n",
        "      elif cabin_cws[4][1] <= random_num < cabin_cws[5][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[4][0]\n",
        "      \n",
        "      elif cabin_cws[5][1] <= random_num < cabin_cws[6][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[5][0]\n",
        "\n",
        "      elif cabin_cws[6][1] <= random_num < cabin_cws[7][1]:\n",
        "        ws['cabin_class'][i] =  cabin_cws[6][0]\n",
        "      else:\n",
        "        ws['cabin_class'][i] = cabin_cws[7][0]\n",
        "\n",
        "  # Perform feature engineering to obtain additional title-info \n",
        "  for i,row in enumerate(ws['Name']):\n",
        "    # Get person's title \n",
        "    ws['title'][i] = row.split(',')[1].split('.')[0]\n",
        "\n",
        "  # Embarked one-hot encoding \n",
        "  embarked_dummies = pd.get_dummies(ws.Embarked, prefix='Embarked')\n",
        "  ws = pd.concat([ws, embarked_dummies], axis=1)\n",
        "\n",
        "  # Person's title one-hot encoding \n",
        "  title_dummies = pd.get_dummies(ws.title, prefix='title')\n",
        "  ws = pd.concat([ws, title_dummies], axis=1)\n",
        "\n",
        "  # Cabin class one-hot encoding \n",
        "  cabin_class_dummies = pd.get_dummies(ws.cabin_class, prefix = 'cabin_class')\n",
        "  ws = pd.concat([ws, cabin_class_dummies], axis = 1)\n",
        "\n",
        "  #Remove unecessary columns \n",
        "  del ws['Name']\n",
        "  del ws['PassengerId']\n",
        "  del ws['title']\n",
        "  del ws['Embarked']\n",
        "  del ws['Cabin']\n",
        "  del ws['Ticket']\n",
        "  del ws['cabin_class']\n",
        "\n",
        "  return ws "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:35.83758Z",
          "iopub.execute_input": "2021-10-21T19:21:35.837838Z",
          "iopub.status.idle": "2021-10-21T19:21:35.866212Z",
          "shell.execute_reply.started": "2021-10-21T19:21:35.837798Z",
          "shell.execute_reply": "2021-10-21T19:21:35.865257Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "DeiOvBFVPBSg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data and create the train / test sets \n",
        "full_ws = data_preprocessing(full_ws)\n",
        "X_train = full_ws[:891]\n",
        "y_train = full_ws['Survived'][:891]\n",
        "X_test = full_ws[891:]\n",
        "del X_train['Survived']\n",
        "del X_test['Survived']\n",
        "\n",
        "\n",
        "print(f'After preprocessing there are {X_train.shape[0]} rows and {X_train.shape[1]} columns in the training data.\\n')\n",
        "print(f'After preprocessing there are {X_test.shape[0]} rows and {X_test.shape[1]} columns in the test data.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:35.867746Z",
          "iopub.execute_input": "2021-10-21T19:21:35.868038Z",
          "iopub.status.idle": "2021-10-21T19:21:36.351716Z",
          "shell.execute_reply.started": "2021-10-21T19:21:35.867994Z",
          "shell.execute_reply": "2021-10-21T19:21:36.350777Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z9nM69EPBSn",
        "outputId": "4b5e98a8-2078-4de8-bcfa-8d210572a3d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After preprocessing there are 891 rows and 36 columns in the training data.\n",
            "\n",
            "After preprocessing there are 418 rows and 36 columns in the test data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are not going to be using any fancy modules for our algorithm \n",
        "# we will be converting our data from pandas dataframe to python lists. \n",
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:36.353655Z",
          "iopub.execute_input": "2021-10-21T19:21:36.353912Z",
          "iopub.status.idle": "2021-10-21T19:21:36.363357Z",
          "shell.execute_reply.started": "2021-10-21T19:21:36.35387Z",
          "shell.execute_reply": "2021-10-21T19:21:36.3625Z"
        },
        "trusted": true,
        "id": "3UX98P4BPBSs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNearestNeighbors():\n",
        "    def __init__(self,k):\n",
        "        self.k = k \n",
        "        self.X_train = None  \n",
        "        self.classes = []\n",
        "    \n",
        "    @staticmethod \n",
        "    def euclidean_distance(row1,row2):\n",
        "      sum = 0\n",
        "      for i,j in zip(row1,row2):\n",
        "        sum += (i-j) ** 2 \n",
        "      distance = sum ** 0.5 \n",
        "      return distance\n",
        "\n",
        "\n",
        "    def _predict(self,classes,distances,k):\n",
        "      # Sort in descending order the distances and retrieve the index \n",
        "      # which maps to their corresponding class \n",
        "      idxs = sorted(range(len(distances)),key = lambda x:distances[x])[:self.k] \n",
        "      # Finding the class for each neighbor  \n",
        "      neighbors = [self.classes[idx] for idx in idxs]\n",
        "      # Choosing the most ocurring class \n",
        "      prediction = Counter(neighbors).most_common(1) # bit of a cheat here :)\n",
        "      return prediction[0][0]\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        self.X_train = X_train\n",
        "        predictions = []\n",
        "        # For each passenger in the training dataset:\n",
        "        for i in range(len(X_train)):\n",
        "            distances = []\n",
        "            classes = []\n",
        "            # Estimate the Euclidean distance for the current \n",
        "            # passenger with all the other passengers \n",
        "            for x,y in zip(X_train,y_train):\n",
        "                distances.append(self.euclidean_distance(X_train[i],x))\n",
        "                # Append the class that corresponds to the current passenger\n",
        "                self.classes.append(y)\n",
        "            # Predict the class for the current passenger \n",
        "            prediction = self._predict(self.classes,distances,self.k)\n",
        "            predictions.append(prediction)\n",
        "        print(f'Utilizing {self.k} groups of nearest-neighbors the training accuracy is at {round(accuracy_score(predictions,y_train)*100,2)}%.')\n",
        "        return predictions   \n",
        "\n",
        "    def predict(self,X_test):\n",
        "        # Predictions for the test data \n",
        "        predictions = []\n",
        "        for i in range(len(X_test)):\n",
        "            distances = []\n",
        "            for x in self.X_train:\n",
        "                distances.append(self.euclidean_distance(X_test[i],x))\n",
        "            prediction = self._predict(self.classes,distances,self.k)\n",
        "            predictions.append(prediction)\n",
        "        return predictions "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:36.364607Z",
          "iopub.execute_input": "2021-10-21T19:21:36.364856Z",
          "iopub.status.idle": "2021-10-21T19:21:36.381568Z",
          "shell.execute_reply.started": "2021-10-21T19:21:36.364819Z",
          "shell.execute_reply": "2021-10-21T19:21:36.380949Z"
        },
        "trusted": true,
        "id": "808psiEwPBSw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing an arbritrary number for k to be 4 \n",
        "KNN = KNearestNeighbors(k=4)\n",
        "KNN.fit(X_train,y_train)\n",
        "predictions = KNN.predict(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T19:21:36.382894Z",
          "iopub.execute_input": "2021-10-21T19:21:36.383672Z",
          "iopub.status.idle": "2021-10-21T19:21:47.248602Z",
          "shell.execute_reply.started": "2021-10-21T19:21:36.383637Z",
          "shell.execute_reply": "2021-10-21T19:21:47.247823Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT5_ja5nPBSz",
        "outputId": "b552a6be-6551-4bea-8cfb-4bc8ca69d770"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizing 4 groups of nearest-neighbors the training accuracy is at 88.78%.\n"
          ]
        }
      ]
    }
  ]
}